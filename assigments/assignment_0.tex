\documentclass[a4paper,10pt, notitlepage]{report}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[shortlabels]{enumitem}
% \usepackage[portuguese]{babel}


% Title Page
\title{Assignment I: Reducing the variance of Monte Carlo Estimators.}
\author{Computational Statistics \\ Instructor: Luiz Max de Carvalho}

\begin{document}
\maketitle


\section*{Background}

Suppose we are interested in an integrable function $\varphi(\cdot)$ and its expectation under a probability distribution with law $\pi$, $I = \int_{\mathbb{X}} \varphi(x) \pi(x) dx$.
Monte Carlo is a large class of sampling algorithms to approximate integrals.
It depends on constructing an estimator
\begin{equation}
\label{eq:MC_est}
 \hat{I}_n = \frac{1}{n} \sum_{i=1}^n \varphi(X_i),
\end{equation}
where $X_i \sim \pi$.
The estimator in~(\ref{eq:MC_est}) can be improved in a number of ways.
In particular, the procedure known as~\textit{Rao-Blackwellisation} allows one to obtain estimators that have lower variance with the same bias, i.e., are more efficient.

\section*{Questions}
\textit{Hint:} I suggest you employ $\varphi(x) = x^2$.
\begin{enumerate}
 \item Show that, if $\pi(x) = \int_{\mathbb{X}} g(x\mid y) h(y)\,dy$, then both
 \begin{align*}
 \hat{I}_n^{\text{MC}} &= \frac{1}{n} \sum_{i=1}^n \varphi(X_i), \, X_i \sim \pi, \: \text{and}\\
 \hat{I}_n^{\text{RB}} &= \frac{1}{n} \sum_{i=1}^n E[\varphi(X) \mid Y_i] , \, Y_i \sim h, 
 \end{align*}
converge to $E_\pi[\varphi(X)]$.
\item For each of the following cases, generate random variables $X_i$ and $Y_i$ and compute the average and variance of the ``vanilla'' Monte Carlo and Rao-Blackwellised estimators:
\begin{enumerate}[(a)]
 \item $X\mid Y \sim \operatorname{Poisson}(Y),\, Y \sim \operatorname{Gamma}(a, b) \implies X \sim \operatorname{Negative-Binomial}$;
  \item $X\mid Y \sim \operatorname{Normal}(0, Y),\, Y \sim \operatorname{Gamma}(a, b) \implies X \sim \operatorname{Generalised-t}$;
   \item $X\mid Y \sim \operatorname{Binomial}(n, Y),\, Y \sim \operatorname{Beta}(a, b) \implies X \sim \operatorname{Beta-Binomial}$;
 \end{enumerate}
 
 \item Propose a Rejection Control (see Question 5 in exercise sheet 1) algorithm to sample from each of the targets in the previous question;
 \item Compute (empirically) the $n_{\text{eff}}$ for the results in the previous question.
 How does the algorithm perform in each case? 
 Do you have to fiddle with the value of $c >0$ in order to obtain better performance?
 Discuss.
 \item Compare the variance of the Rejection Control estimate with the variance of the ``vanilla'' Monte Carlo estimator and the Rao-Blackwellised  estimator;
 \item What other improvements (in terms of variance and bias) would you propose?
 Can you come up with an algorithm that dominates all algorithms studied here for the targets under consideration?
\end{enumerate}

% 
% \bibliographystyle{apalike}
% \bibliography{refs}

\end{document}          
